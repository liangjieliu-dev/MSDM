{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5447a9e5-3b98-45f2-b378-e1924536ab65",
   "metadata": {},
   "source": [
    "# P1\n",
    "\n",
    "1. \n",
    "The original objective maximizes the margin by minimizing $ |\\beta|^{-1} $, which is equivalent to maximizing $ |\\beta| $. To normalize $ |\\beta| = 1 $, the scaling factor is absorbed, and the margin constraint becomes:\n",
    "\n",
    "$$\n",
    "y_i (\\beta^T x_i + \\beta_0) \\geq M,\n",
    "$$\n",
    "\n",
    "where $ M = \\min_i y_i (\\beta^T x_i + \\beta_0) $. This reformulation maintains equivalence.\n",
    "\n",
    "2. \n",
    "From the dual form of the SVM, the weight vector $ \\beta $ is expressed as:\n",
    "\n",
    "$$\n",
    "\\beta = \\sum_{i=1}^n a_i y_i x_i,\n",
    "$$\n",
    "\n",
    "where $ a_i > 0 $ only for support vectors. Substituting $ \\beta $ into the decision function:\n",
    "\n",
    "$$\n",
    "f(x) = \\beta^T x + \\beta_0 = \\left( \\sum_{i=1}^n a_i y_i x_i \\right)^T x + \\beta_0,\n",
    "$$\n",
    "\n",
    "simplifies to:\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i=1}^n a_i \\langle x_i, x \\rangle.\n",
    "$$\n",
    "\n",
    "3. \n",
    "For the RBF (Radial Basis Function) kernel:\n",
    "\n",
    "$$\n",
    "K(x, x’) = \\exp(-\\gamma |x - x’|^2),\n",
    "$$\n",
    "\n",
    "the parameter $ \\gamma $ determines the smoothness of the decision boundary:\n",
    "- High $ \\gamma $: The decision boundary becomes very flexible, fitting tightly to the training data, leading to high variance and low bias (overfitting).\n",
    "- Low $ \\gamma $: The decision boundary becomes smoother and less sensitive to individual data points, resulting in low variance and high bias (underfitting).\n",
    "\n",
    "4. Handling missing data in PCA:\n",
    "\n",
    "    PCA requires complete data for covariance or SVD computation. Missing data can distort results.\n",
    "    \n",
    "    Best preprocessing practices:\n",
    "\n",
    "    1.\tImputation: Replace missing values using:\n",
    "        - Mean, median, or mode of the feature.\n",
    "        - KNN-based imputation.\n",
    "        - Regression-based methods.\n",
    "    2.\tStandardization: Normalize features to have zero mean and unit variance.\n",
    "    3.\tRemoving incomplete rows/columns: If the amount of missing data is small.\n",
    "\n",
    "5. The K-means++ algorithm initializes cluster centers as follows:\n",
    "    1.\tSelect the first center randomly.\n",
    "    2.\tFor each subsequent center, select a point with probability proportional to its squared distance from the nearest existing center.\n",
    "\n",
    "    Advantages over standard K-means:\n",
    "\n",
    "    1.\tReduces the chance of poor initialization.\n",
    "    2.\tSpeeds up convergence.\n",
    "    3.\tImproves final clustering performance by ensuring diverse initial centers.\n",
    "\n",
    "# P2\n",
    "1. Best Parameters: {'n_estimators': 250, 'max_features': 'log2'}, Best CV Error: 0.1868\n",
    "\n",
    "    Validation Confusion Matrix:\n",
    "\n",
    "     4041  153  229  182\n",
    "   \n",
    "     273 2817  153  276\n",
    "   \n",
    "     564  201 1537  355\n",
    "   \n",
    "     237  196  215 4813\n",
    "   \n",
    "    Top 10 Important Keywords:\n",
    "   \n",
    "    'space' 'religion' 'graphics' 'jews' 'team' 'government' 'god' 'car' 'christian' 'windows'\n",
    "\n",
    "3. Best Parameters: {'n_estimators': 250, 'learning_rate': 0.2}, Best CV Error: 0.1765\n",
    "\n",
    "    Validation Confusion Matrix:\n",
    "\n",
    "     4128   77  203  197\n",
    "\n",
    "     285 2747  155  332\n",
    "\n",
    "     565  120 1621  351\n",
    "\n",
    "     225  122  235 4879\n",
    "\n",
    "4. Boosting trees are better.\n",
    "\n",
    "5. Cross-Validation Misclassification Error: 0.2221\n",
    "\n",
    "   Validation Confusion Matrix:\n",
    "\n",
    "     3942   78  308  277\n",
    "\n",
    "     279 2404  235  601\n",
    "\n",
    "     531  123 1542  461\n",
    "\n",
    "     204  147  363 4747\n",
    "\n",
    "6. Cross-Validation Misclassification Error: 0.2385\n",
    "\n",
    "   Validation Confusion Matrix:\n",
    "\n",
    "   3884  617   55   49\n",
    "\n",
    "   159 3234   25  101\n",
    "\n",
    "   569  873  987  228\n",
    "\n",
    "   225  864  108 4264\n",
    "\n",
    "7. XGBoost is the best method.\n",
    "\n",
    "          Model  CV Error\n",
    "\n",
    "  Random Forest  0.186800\n",
    "\n",
    "        XGBoost  0.176518\n",
    "        \n",
    "            LDA  0.222079\n",
    "            \n",
    "            QDA  0.238456\n",
    "\n",
    "# P3\n",
    "All the following times **include the time for cross validation**.\n",
    "\n",
    "1. Best C: 0.01\n",
    "\n",
    "    Misclassification Error: 0.0061\n",
    "\n",
    "    Confusion Matrix:\n",
    "\n",
    "     1251   11\n",
    "\n",
    "        4 1196\n",
    "\n",
    "    Training Time: 1.82 seconds\n",
    "\n",
    "2. Best Parameters: {'C': 1000, 'gamma': 0.001}\n",
    "\n",
    "    Misclassification Error: 0.0049\n",
    "\n",
    "    Confusion Matrix:\n",
    "\n",
    "     1255    7\n",
    "\n",
    "        5 1195\n",
    "\n",
    "    Training Time: 6.41 seconds\n",
    "\n",
    "3.  Binary Model Comparison (3 vs 6)\n",
    "\n",
    "Linear Kernel: Best C=0.01, Error=0.0061, Time=1.82s\n",
    "\n",
    "RBF Kernel: Best C=1000, Gamma=0.001, Error=0.0049, Time=6.41s\n",
    "\n",
    "So Linear Kernel is fast, but RBF Kernel has less Error.\n",
    "\n",
    "4. Best C: 0.1\n",
    "\n",
    "    Misclassification Error: 0.0460\n",
    "    \n",
    "    Confusion Matrix:\n",
    "    \n",
    "     1346    9    1    7\n",
    "     \n",
    "        7 1134   18   26\n",
    "     \n",
    "       18   14 1061   33\n",
    "     \n",
    "       26   17   45 1044\n",
    "     \n",
    "    Training Time: 15.83 seconds\n",
    "\n",
    "5. Best Parameters: {'C': 10, 'gamma': 0.01}\n",
    "\n",
    "    Misclassification Error: 0.0357\n",
    "    \n",
    "    Confusion Matrix:\n",
    "    \n",
    "     1123    0    6    1    0    3    6    0    1    0\n",
    "     \n",
    "        0 1342   12    1    1    0    1    2    3    1\n",
    "        \n",
    "        4    1 1155    6    4    1    0    8    5    1\n",
    "        \n",
    "        0    2   37 1195    0    9    1    7    5    6\n",
    "        \n",
    "        2    2   16    0 1134    2    4    4    0   11\n",
    "        \n",
    "        4    2    9   18    1 1067   11    2   10    2\n",
    "        \n",
    "        4    0   16    0    3    7 1166    0    4    0\n",
    "        \n",
    "        1    2   18    2    6    1    0 1217    0   17\n",
    "        \n",
    "        5    3   10   12    6    7    1    5 1074    9\n",
    "        \n",
    "        4    2   11    3   16    2    0   13    3 1099\n",
    "    \n",
    "    Training Time: 1656.18 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0113dd08-2a26-465e-91d4-95e54e58f5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y after mapping: [0 1 2 3]\n",
      "\n",
      "--- Model Comparison ---\n",
      "\n",
      "--- Random Forest ---\n",
      "Parameters: n_estimators=200, max_features=sqrt, CV Error=0.1899\n",
      "Parameters: n_estimators=200, max_features=log2, CV Error=0.1877\n",
      "Parameters: n_estimators=250, max_features=sqrt, CV Error=0.1904\n",
      "Parameters: n_estimators=250, max_features=log2, CV Error=0.1868\n",
      "Parameters: n_estimators=300, max_features=sqrt, CV Error=0.1906\n",
      "Parameters: n_estimators=300, max_features=log2, CV Error=0.1875\n",
      "Best Parameters: {'n_estimators': 250, 'max_features': 'log2'}, Best CV Error: 0.1868\n",
      "Validation Confusion Matrix:\n",
      " [[4041  153  229  182]\n",
      " [ 273 2817  153  276]\n",
      " [ 564  201 1537  355]\n",
      " [ 237  196  215 4813]]\n",
      "Training Confusion Matrix:\n",
      " [[4370   95   69   71]\n",
      " [ 200 3115   73  131]\n",
      " [ 270  112 2113  162]\n",
      " [ 148  134   93 5086]]\n",
      "Top 10 Important Keywords:\n",
      "['space' 'religion' 'graphics' 'jews' 'team' 'government' 'god' 'car'\n",
      " 'christian' 'windows']\n",
      "\n",
      "--- XGBoost ---\n",
      "Parameters: n_estimators=200, learning_rate=0.1, CV Error=0.1798\n",
      "Parameters: n_estimators=200, learning_rate=0.2, CV Error=0.1766\n",
      "Parameters: n_estimators=200, learning_rate=0.3, CV Error=0.1778\n",
      "Parameters: n_estimators=250, learning_rate=0.1, CV Error=0.1781\n",
      "Parameters: n_estimators=250, learning_rate=0.2, CV Error=0.1765\n",
      "Parameters: n_estimators=250, learning_rate=0.3, CV Error=0.1792\n",
      "Parameters: n_estimators=300, learning_rate=0.1, CV Error=0.1771\n",
      "Parameters: n_estimators=300, learning_rate=0.2, CV Error=0.1766\n",
      "Parameters: n_estimators=300, learning_rate=0.3, CV Error=0.1794\n",
      "Best Parameters: {'n_estimators': 250, 'learning_rate': 0.2}, Best CV Error: 0.1765\n",
      "Validation Confusion Matrix:\n",
      " [[4128   77  203  197]\n",
      " [ 285 2747  155  332]\n",
      " [ 565  120 1621  351]\n",
      " [ 225  122  235 4879]]\n",
      "Training Confusion Matrix:\n",
      " [[4259   61  134  151]\n",
      " [ 254 2850  123  292]\n",
      " [ 464   80 1838  275]\n",
      " [ 192   80  176 5013]]\n",
      "\n",
      "--- Linear Discriminant Analysis ---\n",
      "Cross-Validation Misclassification Error: 0.2221\n",
      "Validation Confusion Matrix:\n",
      " [[3942   78  308  277]\n",
      " [ 279 2404  235  601]\n",
      " [ 531  123 1542  461]\n",
      " [ 204  147  363 4747]]\n",
      "Training Confusion Matrix:\n",
      " [[4018   76  263  248]\n",
      " [ 309 2417  215  578]\n",
      " [ 578  122 1519  438]\n",
      " [ 229  145  346 4741]]\n",
      "\n",
      "--- Quadratic Discriminant Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Misclassification Error: 0.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/liuliangjie/Library/Python/3.9/lib/python/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Confusion Matrix:\n",
      " [[3884  617   55   49]\n",
      " [ 159 3234   25  101]\n",
      " [ 569  873  987  228]\n",
      " [ 225  864  108 4264]]\n",
      "Training Confusion Matrix:\n",
      " [[3908  594   54   49]\n",
      " [ 156 3240   24   99]\n",
      " [ 571  843 1028  215]\n",
      " [ 230  855  108 4268]]\n",
      "\n",
      "--- Summary of Model Performances ---\n",
      "           Model  CV Error\n",
      "0  Random Forest  0.186800\n",
      "1        XGBoost  0.176518\n",
      "2            LDA  0.222079\n",
      "3            QDA  0.238456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier  # 使用 XGBoost\n",
    "\n",
    "# 数据加载和预处理\n",
    "def load_data():\n",
    "    # 加载数据\n",
    "    wordlist = pd.read_csv(\"20newsgroup/wordlist.txt\", header=None, names=[\"word\"])\n",
    "    documents = pd.read_csv(\"20newsgroup/documents.txt\", header=None, sep='\\s+', names=[\"row\", \"col\", \"value\"])\n",
    "    newsgroups = pd.read_csv(\"20newsgroup/newsgroups.txt\", header=None, names=[\"group\"])\n",
    "    groupnames = pd.read_csv(\"20newsgroup/groupnames.txt\", header=None, names=[\"groupname\"])\n",
    "\n",
    "    # 构建稠密矩阵\n",
    "    num_posts = newsgroups.shape[0]  # 行数\n",
    "    num_keywords = wordlist.shape[0]  # 列数\n",
    "    X = np.zeros((num_posts, num_keywords))\n",
    "    for _, row in documents.iterrows():\n",
    "        X[int(row[\"row\"]) - 1, int(row[\"col\"]) - 1] = int(row[\"value\"])\n",
    "\n",
    "    # 修正类别标签为从 0 开始\n",
    "    y = newsgroups[\"group\"].values\n",
    "    y = y - y.min()  # 将最小值变为 0（例如 [1, 2, 3, 4] -> [0, 1, 2, 3]）\n",
    "\n",
    "    return X, y, wordlist, groupnames\n",
    "\n",
    "# 加载数据\n",
    "X, y, wordlist, groupnames = load_data()\n",
    "print(\"Unique classes in y after mapping:\", np.unique(y))  # 确保类别正确\n",
    "\n",
    "# 随机森林分类器（含调参）\n",
    "def random_forest_classifier(X, y):\n",
    "    print(\"\\n--- Random Forest ---\")\n",
    "    best_score = 1.0\n",
    "    best_params = {}\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for n_estimators in [200, 250, 300]:\n",
    "        for max_features in ['sqrt', 'log2']:\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, random_state=42)\n",
    "            cv_scores = cross_val_score(rf, X, y, cv=skf, scoring='accuracy')\n",
    "            cv_error = 1 - np.mean(cv_scores)\n",
    "            print(f\"Parameters: n_estimators={n_estimators}, max_features={max_features}, CV Error={cv_error:.4f}\")\n",
    "            if cv_error < best_score:\n",
    "                best_score = cv_error\n",
    "                best_params = {\"n_estimators\": n_estimators, \"max_features\": max_features}\n",
    "                best_model = rf\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}, Best CV Error: {best_score:.4f}\")\n",
    "    \n",
    "    # 交叉验证阶段混淆矩阵\n",
    "    y_pred_cv = cross_val_predict(best_model, X, y, cv=skf)\n",
    "    conf_matrix_cv = confusion_matrix(y, y_pred_cv)\n",
    "    print(\"Validation Confusion Matrix:\\n\", conf_matrix_cv)\n",
    "\n",
    "    # 使用最佳参数训练最终模型并在训练集上计算混淆矩阵\n",
    "    best_model.fit(X, y)\n",
    "    y_pred_train = best_model.predict(X)\n",
    "    conf_matrix_train = confusion_matrix(y, y_pred_train)\n",
    "    print(\"Training Confusion Matrix:\\n\", conf_matrix_train)\n",
    "    \n",
    "    # 输出特征重要性\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    important_features = np.argsort(feature_importances)[-10:]\n",
    "    print(\"Top 10 Important Keywords:\")\n",
    "    print(wordlist.iloc[important_features][\"word\"].values)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "# XGBoost 分类器（含调参）\n",
    "def xgboost_classifier(X, y):\n",
    "    print(\"\\n--- XGBoost ---\")\n",
    "    best_score = 1.0\n",
    "    best_params = {}\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for n_estimators in [200, 250, 300]:\n",
    "        for learning_rate in [0.1, 0.2, 0.3]:\n",
    "            xgb = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, objective='multi:softmax', random_state=42)\n",
    "            cv_scores = cross_val_score(xgb, X, y, cv=skf, scoring='accuracy')\n",
    "            cv_error = 1 - np.mean(cv_scores)\n",
    "            print(f\"Parameters: n_estimators={n_estimators}, learning_rate={learning_rate}, CV Error={cv_error:.4f}\")\n",
    "            if cv_error < best_score:\n",
    "                best_score = cv_error\n",
    "                best_params = {\"n_estimators\": n_estimators, \"learning_rate\": learning_rate}\n",
    "                best_model = xgb\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}, Best CV Error: {best_score:.4f}\")\n",
    "    \n",
    "    # 交叉验证阶段混淆矩阵\n",
    "    y_pred_cv = cross_val_predict(best_model, X, y, cv=skf)\n",
    "    conf_matrix_cv = confusion_matrix(y, y_pred_cv)\n",
    "    print(\"Validation Confusion Matrix:\\n\", conf_matrix_cv)\n",
    "\n",
    "    # 使用最佳参数训练最终模型并在训练集上计算混淆矩阵\n",
    "    best_model.fit(X, y)\n",
    "    y_pred_train = best_model.predict(X)\n",
    "    conf_matrix_train = confusion_matrix(y, y_pred_train)\n",
    "    print(\"Training Confusion Matrix:\\n\", conf_matrix_train)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "# 线性判别分析\n",
    "def lda_classifier(X, y):\n",
    "    print(\"\\n--- Linear Discriminant Analysis ---\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "    cv_scores = cross_val_score(lda, X, y, cv=skf, scoring='accuracy')\n",
    "    cv_error = 1 - np.mean(cv_scores)\n",
    "    print(f\"Cross-Validation Misclassification Error: {cv_error:.4f}\")  # 打印 CV Error\n",
    "\n",
    "    # 交叉验证阶段混淆矩阵\n",
    "    y_pred_cv = cross_val_predict(lda, X, y, cv=skf)\n",
    "    conf_matrix_cv = confusion_matrix(y, y_pred_cv)\n",
    "    print(\"Validation Confusion Matrix:\\n\", conf_matrix_cv)\n",
    "    \n",
    "    # 模型训练与训练集评估\n",
    "    lda.fit(X, y)\n",
    "    y_pred_train = lda.predict(X)\n",
    "    conf_matrix_train = confusion_matrix(y, y_pred_train)\n",
    "    print(\"Training Confusion Matrix:\\n\", conf_matrix_train)\n",
    "    \n",
    "    return cv_error\n",
    "\n",
    "# 二次判别分析\n",
    "def qda_classifier(X, y):\n",
    "    print(\"\\n--- Quadratic Discriminant Analysis ---\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    qda = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
    "    cv_scores = cross_val_score(qda, X, y, cv=skf, scoring='accuracy')\n",
    "    cv_error = 1 - np.mean(cv_scores)\n",
    "    print(f\"Cross-Validation Misclassification Error: {cv_error:.4f}\")  # 打印 CV Error\n",
    "\n",
    "    # 交叉验证阶段混淆矩阵\n",
    "    y_pred_cv = cross_val_predict(qda, X, y, cv=skf)\n",
    "    conf_matrix_cv = confusion_matrix(y, y_pred_cv)\n",
    "    print(\"Validation Confusion Matrix:\\n\", conf_matrix_cv)\n",
    "    \n",
    "    # 模型训练与训练集评估\n",
    "    qda.fit(X, y)\n",
    "    y_pred_train = qda.predict(X)\n",
    "    conf_matrix_train = confusion_matrix(y, y_pred_train)\n",
    "    print(\"Training Confusion Matrix:\\n\", conf_matrix_train)\n",
    "    \n",
    "    return cv_error\n",
    "\n",
    "# 比较所有模型\n",
    "def compare_models(X, y):\n",
    "    print(\"\\n--- Model Comparison ---\")\n",
    "    rf_error = random_forest_classifier(X, y)\n",
    "    xgb_error = xgboost_classifier(X, y)\n",
    "    lda_error = lda_classifier(X, y)\n",
    "    qda_error = qda_classifier(X, y)\n",
    "    \n",
    "    models = [\"Random Forest\", \"XGBoost\", \"LDA\", \"QDA\"]\n",
    "    cv_errors = [rf_error, xgb_error, lda_error, qda_error]\n",
    "    \n",
    "    print(\"\\n--- Summary of Model Performances ---\")\n",
    "    results = pd.DataFrame({\n",
    "        \"Model\": models,\n",
    "        \"CV Error\": cv_errors\n",
    "    })\n",
    "    print(results)\n",
    "\n",
    "# 执行所有模型\n",
    "compare_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef703580-0d4d-49ec-b089-0691adcb2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Binary Classification (3 vs 6, Linear Kernel) ---\n",
      "Training Samples: 6026, Test Samples: 2462\n",
      "Best C: 0.01\n",
      "Misclassification Error: 0.0061\n",
      "Confusion Matrix:\n",
      " [[1251   11]\n",
      " [   4 1196]]\n",
      "Training Time: 1.82 seconds\n",
      "\n",
      "--- Binary Classification (3 vs 6, RBF Kernel) ---\n",
      "Training Samples: 6026, Test Samples: 2462\n",
      "Best Parameters: {'C': 1000, 'gamma': 0.001}\n",
      "Misclassification Error: 0.0049\n",
      "Confusion Matrix:\n",
      " [[1255    7]\n",
      " [   5 1195]]\n",
      "Training Time: 6.41 seconds\n",
      "\n",
      "--- Binary Model Comparison (3 vs 6) ---\n",
      "Linear Kernel: Best C=0.01, Error=0.0061, Time=1.82s\n",
      "RBF Kernel: Best C=1000, Gamma=0.001, Error=0.0049, Time=6.41s\n",
      "\n",
      "--- Multi-Class Classification (1, 2, 5, 8, Linear Kernel) ---\n",
      "Training Samples: 11913, Test Samples: 4806\n",
      "Best C: 0.1\n",
      "Misclassification Error: 0.0460\n",
      "Confusion Matrix:\n",
      " [[1346    9    1    7]\n",
      " [   7 1134   18   26]\n",
      " [  18   14 1061   33]\n",
      " [  26   17   45 1044]]\n",
      "Training Time: 15.83 seconds\n",
      "\n",
      "--- Multi-Class Classification (All 10 Digits, RBF Kernel) ---\n",
      "Best Parameters: {'C': 10, 'gamma': 0.01}\n",
      "Misclassification Error: 0.0357\n",
      "Confusion Matrix:\n",
      " [[1123    0    6    1    0    3    6    0    1    0]\n",
      " [   0 1342   12    1    1    0    1    2    3    1]\n",
      " [   4    1 1155    6    4    1    0    8    5    1]\n",
      " [   0    2   37 1195    0    9    1    7    5    6]\n",
      " [   2    2   16    0 1134    2    4    4    0   11]\n",
      " [   4    2    9   18    1 1067   11    2   10    2]\n",
      " [   4    0   16    0    3    7 1166    0    4    0]\n",
      " [   1    2   18    2    6    1    0 1217    0   17]\n",
      " [   5    3   10   12    6    7    1    5 1074    9]\n",
      " [   4    2   11    3   16    2    0   13    3 1099]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98      1140\n",
      "         1.0       0.99      0.98      0.99      1363\n",
      "         2.0       0.90      0.97      0.93      1185\n",
      "         3.0       0.97      0.95      0.96      1262\n",
      "         4.0       0.97      0.97      0.97      1175\n",
      "         5.0       0.97      0.95      0.96      1126\n",
      "         6.0       0.98      0.97      0.98      1200\n",
      "         7.0       0.97      0.96      0.97      1264\n",
      "         8.0       0.97      0.95      0.96      1132\n",
      "         9.0       0.96      0.95      0.96      1153\n",
      "\n",
      "    accuracy                           0.96     12000\n",
      "   macro avg       0.96      0.96      0.96     12000\n",
      "weighted avg       0.96      0.96      0.96     12000\n",
      "\n",
      "Training Time: 1656.18 seconds\n",
      "\n",
      "--- Model Performance Summary ---\n",
      "                               Task        C     error         time  gamma\n",
      "0           Binary (3 vs 6, Linear)     0.01  0.006093     1.815479    NaN\n",
      "1              Binary (3 vs 6, RBF)  1000.00  0.004874     6.412924  0.001\n",
      "2  Multi-Class (1, 2, 5, 8, Linear)     0.10  0.045984    15.833344    NaN\n",
      "3               Full 10-Class (RBF)    10.00  0.035667  1656.184587  0.010\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 数据加载和预处理\n",
    "def load_data():\n",
    "    train_data = pd.read_csv(\"MNIST/train_resized.csv\").values\n",
    "    test_data = pd.read_csv(\"MNIST/test_resized.csv\").values\n",
    "\n",
    "    X_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "    X_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# 添加标准化器\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 二分类任务（线性核）\n",
    "def svm_linear_binary(X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n--- Binary Classification (3 vs 6, Linear Kernel) ---\")\n",
    "    train_idx = np.isin(y_train, [3, 6])\n",
    "    test_idx = np.isin(y_test, [3, 6])\n",
    "    X_train_bin, y_train_bin = X_train[train_idx], y_train[train_idx]\n",
    "    X_test_bin, y_test_bin = X_test[test_idx], y_test[test_idx]\n",
    "    y_train_bin = (y_train_bin == 6).astype(int)\n",
    "    y_test_bin = (y_test_bin == 6).astype(int)\n",
    "\n",
    "    print(f\"Training Samples: {X_train_bin.shape[0]}, Test Samples: {X_test_bin.shape[0]}\")\n",
    "\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1]}\n",
    "    clf = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5)\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train_bin, y_train_bin)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    best_C = clf.best_params_['C']\n",
    "    y_pred = clf.predict(X_test_bin)\n",
    "    misclassification_error = 1 - accuracy_score(y_test_bin, y_pred)\n",
    "\n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Misclassification Error: {misclassification_error:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_bin, y_pred))\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "    return {\"C\": best_C, \"error\": misclassification_error, \"time\": train_time}\n",
    "\n",
    "# 二分类任务（径向基核）\n",
    "def svm_rbf_binary(X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n--- Binary Classification (3 vs 6, RBF Kernel) ---\")\n",
    "    train_idx = np.isin(y_train, [3, 6])\n",
    "    test_idx = np.isin(y_test, [3, 6])\n",
    "    X_train_bin, y_train_bin = X_train[train_idx], y_train[train_idx]\n",
    "    X_test_bin, y_test_bin = X_test[test_idx], y_test[test_idx]\n",
    "    y_train_bin = (y_train_bin == 6).astype(int)\n",
    "    y_test_bin = (y_test_bin == 6).astype(int)\n",
    "\n",
    "    print(f\"Training Samples: {X_train_bin.shape[0]}, Test Samples: {X_test_bin.shape[0]}\")\n",
    "\n",
    "    param_grid = {'C': [100, 1000, 10000], 'gamma': [0.0001, 0.001, 0.01]}\n",
    "    clf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train_bin, y_train_bin)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    best_params = clf.best_params_\n",
    "    y_pred = clf.predict(X_test_bin)\n",
    "    misclassification_error = 1 - accuracy_score(y_test_bin, y_pred)\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Misclassification Error: {misclassification_error:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_bin, y_pred))\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "    return {\"C\": best_params[\"C\"], \"gamma\": best_params[\"gamma\"], \"error\": misclassification_error, \"time\": train_time}\n",
    "\n",
    "# 多分类任务（1, 2, 5, 8 的分类，线性核）\n",
    "def svm_linear_multi(X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n--- Multi-Class Classification (1, 2, 5, 8, Linear Kernel) ---\")\n",
    "    train_idx = np.isin(y_train, [1, 2, 5, 8])\n",
    "    test_idx = np.isin(y_test, [1, 2, 5, 8])\n",
    "    X_train_multi, y_train_multi = X_train[train_idx], y_train[train_idx]\n",
    "    X_test_multi, y_test_multi = X_test[test_idx], y_test[test_idx]\n",
    "\n",
    "    print(f\"Training Samples: {X_train_multi.shape[0]}, Test Samples: {X_test_multi.shape[0]}\")\n",
    "\n",
    "    param_grid = {'C': [0.01, 0.1, 1]}\n",
    "    clf = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5)\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train_multi, y_train_multi)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    best_C = clf.best_params_['C']\n",
    "    y_pred = clf.predict(X_test_multi)\n",
    "    misclassification_error = 1 - accuracy_score(y_test_multi, y_pred)\n",
    "\n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Misclassification Error: {misclassification_error:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_multi, y_pred))\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "    return {\"C\": best_C, \"error\": misclassification_error, \"time\": train_time}\n",
    "\n",
    "# 多分类任务（全10类分类，径向基核）\n",
    "def svm_full(X_train, y_train, X_test, y_test):\n",
    "    print(\"\\n--- Multi-Class Classification (All 10 Digits, RBF Kernel) ---\")\n",
    "    param_grid = {'C': [1, 10, 100], 'gamma': [0.001, 0.01, 0.1]}  # 更新 C 参数范围\n",
    "    clf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "\n",
    "    best_params = clf.best_params_\n",
    "    y_pred = clf.predict(X_test)\n",
    "    misclassification_error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Misclassification Error: {misclassification_error:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(f\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "    return {\"C\": best_params[\"C\"], \"gamma\": best_params[\"gamma\"], \"error\": misclassification_error, \"time\": train_time}\n",
    "\n",
    "# 模型性能比较\n",
    "def compare_binary_models(linear_results, rbf_results):\n",
    "    print(\"\\n--- Binary Model Comparison (3 vs 6) ---\")\n",
    "    print(f\"Linear Kernel: Best C={linear_results['C']}, Error={linear_results['error']:.4f}, Time={linear_results['time']:.2f}s\")\n",
    "    print(f\"RBF Kernel: Best C={rbf_results['C']}, Gamma={rbf_results['gamma']}, Error={rbf_results['error']:.4f}, Time={rbf_results['time']:.2f}s\")\n",
    "\n",
    "# 汇总所有模型的结果\n",
    "def summarize_results(results):\n",
    "    print(\"\\n--- Model Performance Summary ---\")\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "\n",
    "# 执行所有任务\n",
    "results = []\n",
    "linear_binary = svm_linear_binary(X_train, y_train, X_test, y_test)\n",
    "results.append({\"Task\": \"Binary (3 vs 6, Linear)\", **linear_binary})\n",
    "rbf_binary = svm_rbf_binary(X_train, y_train, X_test, y_test)\n",
    "results.append({\"Task\": \"Binary (3 vs 6, RBF)\", **rbf_binary})\n",
    "compare_binary_models(linear_binary, rbf_binary)\n",
    "\n",
    "linear_multi = svm_linear_multi(X_train, y_train, X_test, y_test)\n",
    "results.append({\"Task\": \"Multi-Class (1, 2, 5, 8, Linear)\", **linear_multi})\n",
    "full_class = svm_full(X_train, y_train, X_test, y_test)\n",
    "results.append({\"Task\": \"Full 10-Class (RBF)\", **full_class})\n",
    "\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930d80c-4511-40f1-a428-701a8d5229f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
